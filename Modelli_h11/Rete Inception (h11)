from keras.models import Sequential, Model
from keras.layers import Dense, Conv2D, Flatten, Activation, Dropout, Input, concatenate
from keras.layers import BatchNormalization
from keras import regularizers
from keras.layers import Layer
from keras.callbacks import ReduceLROnPlateau
import matplotlib.pyplot as plt
import random
import keras.backend as K

data_X=cicy3['Config. matrix']
data_y=cicy3['H11']

test_index = random.sample([k for k in range(len(data_X))], round(len(data_X)/10) )#tengo da parte un 10% del dataset
train_val_index = list(set([k for k in range(len(data_X))]) - set(test_index))

perc = 80 #scelgo la percentuale del dataset in train

train = random.sample(train_val_index, round(len(data_X)*(perc/100)))
val = list(set(train_val_index)-set(train))

data_train_X=np.array([data_X[a] for a in train])
data_train_y=np.array([data_y[a] for a in train])

data_val_X=np.array([data_X[a] for a in val])
data_val_y=np.array([data_y[a] for a in val])

data_test_X=np.array([data_X[a] for a in test_index])
data_test_y=np.array([data_y[a] for a in test_index])


#Non capisco se le regolarizzazioni L1 e L2 vanno applicate dopo un blocco o ad ogni strato di convoluzione (nel primo
#   caso non saprei come applicarle) e che tipo di regolarizzazione devo mettere (kernel, bias o activity)

#Applicando la regolarizzazione sul kernel e sul bias ottengo tra 96% e 99% di accuratezza con 80% di training set e 500 epoche

#Layer 1
net=Sequential()
input_matrix=Input(shape=(12, 15, 1))
layer1_a=Conv2D(32, kernel_size=(12, 1), kernel_regularizer=regularizers.L1L2(l1=1e-4, l2=1e-4),
                bias_regularizer=regularizers.L1L2(l1=1e-4, l2=1e-4), padding='same', activation='relu')(input_matrix)
layer1_b=Conv2D(32, kernel_size=(1, 15), kernel_regularizer=regularizers.L1L2(l1=1e-4, l2=1e-4),
                bias_regularizer=regularizers.L1L2(l1=1e-4, l2=1e-4), padding='same', activation='relu')(input_matrix)
layer1=concatenate([layer1_a, layer1_b])
net=BatchNormalization(momentum=0.99)(layer1)



#Layer 2
layer2_a=Conv2D(64, kernel_size=(12, 1), kernel_regularizer=regularizers.L1L2(l1=1e-4, l2=1e-4),
                bias_regularizer=regularizers.L1L2(l1=1e-4, l2=1e-4), padding='same', activation='relu')(net)
layer2_b=Conv2D(64, kernel_size=(1, 15), kernel_regularizer=regularizers.L1L2(l1=1e-4, l2=1e-4),
                bias_regularizer=regularizers.L1L2(l1=1e-4, l2=1e-4), padding='same', activation='relu')(net)
layer2=concatenate([layer2_a, layer2_b])
net=BatchNormalization(momentum=0.99)(layer2)



#Layer 3
layer3_a=Conv2D(32, kernel_size=(12, 1), kernel_regularizer=regularizers.L1L2(l1=1e-4, l2=1e-4),
                bias_regularizer=regularizers.L1L2(l1=1e-4, l2=1e-4), padding='same', activation='relu')(net)
layer3_b=Conv2D(32, kernel_size=(1, 15), kernel_regularizer=regularizers.L1L2(l1=1e-4, l2=1e-4),
                bias_regularizer=regularizers.L1L2(l1=1e-4, l2=1e-4), padding='same', activation='relu')(net)
layer3=concatenate([layer3_a, layer3_b])
net=BatchNormalization(momentum=0.99)(layer3)


#Output Layer
net=Dropout(0.2)(net)
net=Flatten()(net)
output=Dense(1, activation='relu', kernel_regularizer=regularizers.L1L2(l1=1e-4, l2=1e-4))(net)


reduce_lr = ReduceLROnPlateau(monitor="val_loss", factor=0.3, patience=75)


model=Model(inputs=input_matrix, outputs=output)
model.compile(loss='mean_squared_error', optimizer = 'adam')

flat_data_train_X=data_train_X.reshape(len(data_train_X)*12*15)
data_train_X_s = flat_data_train_X.reshape(len(data_train_X), 12, 15, 1)

flat_data_val_X=data_val_X.reshape(len(data_val_X)*12*15)
data_val_X_s = flat_data_val_X.reshape(len(data_val_X), 12, 15, 1)

flat_data_test_X=data_test_X.reshape(len(data_test_X)*12*15)
data_test_X_s = flat_data_test_X.reshape(len(data_test_X), 12, 15, 1)

epochs=1500 #numero di epoche

history = model.fit(data_train_X_s, data_train_y, batch_size=32, epochs=epochs, verbose=1, 
                     validation_data=(data_val_X_s, data_val_y), callbacks=[reduce_lr])
