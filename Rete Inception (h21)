from keras.models import Sequential, Model
from keras.layers import Dense, Conv2D, Flatten, Activation, Dropout, Input, concatenate
from keras.layers import BatchNormalization
from keras import regularizers
from keras.layers import Layer
from keras.callbacks import ReduceLROnPlateau
import random
import keras.backend as K
#from sklearn.model_selection import KFold

data_X=cicy3['Config. matrix']
data_y=cicy3['H21']

test_index = random.sample([k for k in range(len(data_X))], round(len(data_X)/10) )#tengo da parte un 10% del dataset
train_val_index = list(set([k for k in range(len(data_X))]) - set(test_index))

perc = 80 #percentuale di training

train = random.sample(train_val_index, round(len(data_X)*(perc/100)))
val = list(set(train_val_index)-set(train))

data_train_X=np.array([data_X[a] for a in train])
data_train_y=np.array([data_y[a] for a in train])

data_val_X=np.array([data_X[a] for a in val])
data_val_y=np.array([data_y[a] for a in val])

data_test_X=np.array([data_X[a] for a in test_index])
data_test_y=np.array([data_y[a] for a in test_index])


#Layer 1
net=Sequential()
input_matrix=Input(shape=(12, 15, 1))
layer1_a=Conv2D(128, kernel_size=(12, 1), kernel_regularizer=regularizers.L1L2(l1=1e-4, l2=1e-4),
                bias_regularizer=regularizers.L1L2(l1=1e-4, l2=1e-4), padding='same', activation='relu')(input_matrix)
layer1_b=Conv2D(128, kernel_size=(1, 15), kernel_regularizer=regularizers.L1L2(l1=1e-4, l2=1e-4),
                bias_regularizer=regularizers.L1L2(l1=1e-4, l2=1e-4), padding='same', activation='relu')(input_matrix)
layer1=concatenate([layer1_a, layer1_b])
net=BatchNormalization(momentum=0.99)(layer1)



#Layer 2
layer2_a=Conv2D(128, kernel_size=(12, 1), kernel_regularizer=regularizers.L1L2(l1=1e-4, l2=1e-4),
                bias_regularizer=regularizers.L1L2(l1=1e-4, l2=1e-4), padding='same', activation='relu')(net)
layer2_b=Conv2D(128, kernel_size=(1, 15), kernel_regularizer=regularizers.L1L2(l1=1e-4, l2=1e-4),
                bias_regularizer=regularizers.L1L2(l1=1e-4, l2=1e-4), padding='same', activation='relu')(net)
layer2=concatenate([layer2_a, layer2_b])
net=BatchNormalization(momentum=0.99)(layer2)



#Layer 3
layer3_a=Conv2D(64, kernel_size=(12, 1), kernel_regularizer=regularizers.L1L2(l1=1e-4, l2=1e-4),
                bias_regularizer=regularizers.L1L2(l1=1e-4, l2=1e-4), padding='same', activation='relu')(net)
layer3_b=Conv2D(64, kernel_size=(1, 15), kernel_regularizer=regularizers.L1L2(l1=1e-4, l2=1e-4),
                bias_regularizer=regularizers.L1L2(l1=1e-4, l2=1e-4), padding='same', activation='relu')(net)
layer3=concatenate([layer3_a, layer3_b])
net=BatchNormalization(momentum=0.99)(layer3)


#Output Layer
net=Dropout(0.2)(net)
net=Flatten()(net)
output=Dense(1, activation='relu', kernel_regularizer=regularizers.L1L2(l1=1e-4, l2=1e-4), 
             bias_regularizer=regularizers.L1L2(l1=1e-4, l2=1e-4))(net)


reduce_lr = ReduceLROnPlateau(monitor="val_loss", factor=0.3, patience=50)


model=Model(inputs=input_matrix, outputs=output)
model.compile(loss='mean_squared_error', optimizer = 'adam')

flat_data_train_X=data_train_X.reshape(len(data_train_X)*12*15)
data_train_X_s = flat_data_train_X.reshape(len(data_train_X), 12, 15, 1)

flat_data_val_X=data_val_X.reshape(len(data_val_X)*12*15)
data_val_X_s = flat_data_val_X.reshape(len(data_val_X), 12, 15, 1)

flat_data_test_X=data_test_X.reshape(len(data_test_X)*12*15)
data_test_X_s = flat_data_test_X.reshape(len(data_test_X), 12, 15, 1)

epochs=1500

history = model.fit(data_train_X_s, data_train_y, batch_size=32, epochs=epochs, verbose=1, 
                     validation_data=(data_val_X_s, data_val_y), callbacks=[reduce_lr])
